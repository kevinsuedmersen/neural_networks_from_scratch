# Neural Networks from Scratch
This repository aims to derive and implement equations for training a neural networks from scratch. First, I will attempt to derive equations for forward- as well as backward propagation in scalar form for a single training example. Then, I will extend these equations to a *matrix-based*  approach for a single training example, and finally, I will extend it to a matrix-based approach for processing `batch_size` examples at once. After implementing the necessary Python code, I will test the network's performance on the MNIST hand-written digits dataset and compare its performance with famous deep learning libraries such as TensorFlow. 

# Table of contents

[TOC]

# Forward Propagation

TODO: 

- Picture of neural network with notation explained
- Formula for activation j in layer l in scalar form
- matrix-based form 
- forward-propagation to layer L
- computation of cost function
